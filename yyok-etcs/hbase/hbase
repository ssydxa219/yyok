export JAVA_HOME=/usr/java/jdk1.8.0_192-amd64
export HBASE_LOG_DIR=/data/local/hbase/log
export HBASE_CONF_DIR=/ddhome/bin/hbase/conf

export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS $HBASE_JMX_BASE -Xmx6000m -Xms6000m -Xmn2250m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:CMSInitiatingOccupancyFraction=70 -Xloggc:/d1/local/hbase/logs/gc.log"


优化配置

 1.垃圾回收优化

Java本身提供了垃圾回收机制，依靠JRE对程序行为的各种假设进行垃圾回收，但是HBase支持海量数据持续入库，非常占用内存，因此繁重的负载会迫使内存分配策略无法安全地依赖于JRE的判断：需要调整JRE的参数来调整垃圾回收策略。有关java内存回收机制的问题具体请参考：http://my.oschina.net/sunnywu/blog/332870。

（1）HBASE_OPTS或者HBASE_REGIONSERVER_OPT变量来设置垃圾回收的选项，后面一般是用于配置RegionServer的，需要在每个子节点的HBASE_OPTS文件中进行配置。

1）首先是设置新生代大小的参数，不能过小，过小则导致年轻代过快成为老生代，引起老生代产生内存随便。同样不能过大，过大导致所有的JAVA进程停止时间长。-XX:MaxNewSize=256m-XX:NewSize=256m 这两个可以合并成为-Xmn256m这一个配置来完成。

2）其次是设置垃圾回收策略：-XX:+UseParNewGC -XX:+UseConcMarkSweepGC也叫收集器设置。

3）设置CMS的值，占比多少时，开始并发标记和清扫检查。-XX:CMSInitiatingOccupancyFraction=70

4）打印垃圾回收信息：-verbose:gc -XX: +PrintGCDetails -XX:+PrintGCTimeStamps

-Xloggc:$HBASE_HOME/logs/gc-$(hostname)-hbase.log

最终可以得到：HBASE_REGIONSERVER_OPT="-Xmx8g -Xms8g –Xmn256m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC  \

-XX:CMSInitiatingOccupancyFraction=70   -verbose:gc \

-XX:+PrintGCDetails -XX:+PrintGCTimeStamps \

-Xloggc:$HBASE_HOME/logs/gc-$(hostname)-hbase.log

（2）hbase.hregion.memstore.mslab.enabled默认值：true，这个是在hbase-site.xml中进行配置的值。

说明：减少因内存碎片导致的Full GC，提高整体性能。

2.启用压缩，详情自行搜索，暂时未曾尝试，后面持续更新。

3.优化Region拆分合并以及与拆分Region

（1）hbase.hregion.max.filesize默认为256M（在hbase-site.xml中进行配置），当region达到这个阈值时，会自动拆分。可以把这个值设的无限大，则可以关闭HBase自动管理拆分，手动运行命令来进行region拆分，这样可以在不同的region上交错运行，分散I/O负载。

（2）预拆分region

用户可以在建表的时候就制定好预设定的region，这样就可以避免后期region自动拆分造成I/O负载。

4.客户端入库调优

（1）用户在编写程序入库时，HBase的自动刷写是默认开启的，即用户每一次put都会提交到HBase server进行一次刷写，如果需要高速插入数据，则会造成I/O负载过重。在这里可以关闭自动刷写功能，setAutoFlush(false)。如此，put实例会先写到一个缓存中，这个缓存的大小通过hbase.client.write.buffer这个值来设定缓存区，当缓存区被填满之后才会被送出。如果想要显示刷写数据，可以调用flushCommits()方法。

此处引申：采取这个方法要估算服务器端内存占用则可以：hbase.client.write.buffer*hbase.regionserver.handler.count得出内存情况。

（2）第二个方法，是关闭每次put上的WAL（writeToWAL(flase)）这样可以刷写数据前，不需要预写日志，但是如果数据重要的话建议不要关闭。

（3）hbase.client.scanner.caching：默认为1

这是设计客户端读取数据的配置调优，在hbase-site.xml中进行配置，代表scanner一次缓存多少数据（从服务器一次抓取多少数据来scan）默认的太小，但是对于大文件，值不应太大。

（4）hbase.regionserver.lease.period默认值：60000

说明：客户端租用HRegion server 期限，即超时阀值。

调优：这个配合hbase.client.scanner.caching使用，如果内存够大，但是取出较多数据后计算过程较长，可能超过这个阈值，适当可设置较长的响应时间以防被认为宕机。

（5）还有诸多实践，如设置过滤器，扫描缓存等，指定行扫描等多种客户端调优方案，需要在实践中慢慢挖掘。

5.HBase配置文件

上面涉及到的调优内容或多或少在HBase配置文件中都有所涉及，因此，下面的配置不涵盖上面已有的配置。

(1) zookeeper.session.timeout（默认3分钟）

ZK的超期参数，默认配置为3分钟，在生产环境上建议减小这个值在1分钟或更小。

设置原则：这个值越小，当RS故障时Hmaster获知越快，Hlog分裂和region 部署越快，集群恢复时间越短。 但是，设置这个值得原则是留足够的时间进行GC回收，否则会导致频繁的RS宕机。一般就做默认即可

（2）hbase.regionserver.handler.count（默认10）

对于大负载的put（达到了M范围）或是大范围的Scan操作，handler数目不易过大，易造成OOM。 对于小负载的put或是get，delete等操作，handler数要适当调大。根据上面的原则，要看我们的业务的情况来设置。（具体情况具体分析）。

（3）HBASE_HEAPSIZE（hbase-env.sh中配置）

我的前两篇文章Memstoresize40%（默认） blockcache 20%（默认）就是依据这个而成的，总体HBase内存配置。设到机器内存的1/2即可。

（4）选择使用压缩算法，目前HBase默认支持的压缩算法包括GZ，LZO以及snappy（hbase-site.xml中配置）

（5）hbase.hregion.max.filesize默认256M

上面说过了，hbase自动拆分region的阈值，可以设大或者无限大，无限大需要手动拆分region，懒的人别这样。

（6）hbase.hregion.memstore.flush.size

单个region内所有的memstore大小总和超过指定值时，flush该region的所有memstore。

（7）hbase.hstore.blockingStoreFiles  默认值：7

说明：在flush时，当一个region中的Store（CoulmnFamily）内有超过7个storefile时，则block所有的写请求进行compaction，以减少storefile数量。

调优：block写请求会严重影响当前regionServer的响应时间，但过多的storefile也会影响读性能。从实际应用来看，为了获取较平滑的响应时间，可将值设为无限大。如果能容忍响应时间出现较大的波峰波谷，那么默认或根据自身场景调整即可。

（8）hbase.hregion.memstore.block.multiplier默认值：2

说明：当一个region里总的memstore占用内存大小超过hbase.hregion.memstore.flush.size两倍的大小时，block该region的所有请求，进行flush，释放内存。

虽然我们设置了region所占用的memstores总内存大小，比如64M，但想象一下，在最后63.9M的时候，我Put了一个200M的数据，此时memstore的大小会瞬间暴涨到超过预期的hbase.hregion.memstore.flush.size的几倍。这个参数的作用是当memstore的大小增至超过hbase.hregion.memstore.flush.size 2倍时，block所有请求，遏制风险进一步扩大。

调优： 这个参数的默认值还是比较靠谱的。如果你预估你的正常应用场景（不包括异常）不会出现突发写或写的量可控，那么保持默认值即可。如果正常情况下，你的写请求量就会经常暴长到正常的几倍，那么你应该调大这个倍数并调整其他参数值，比如hfile.block.cache.size和hbase.regionserver.global.memstore.upperLimit/lowerLimit，以预留更多内存，防止HBase server OOM。

（9）hbase.regionserver.global.memstore.upperLimit：默认40%

当ReigonServer内所有region的memstores所占用内存总和达到heap的40%时，HBase会强制block所有的更新并flush这些region以释放所有memstore占用的内存。

hbase.regionserver.global.memstore.lowerLimit：默认35%

同upperLimit，只不过lowerLimit在所有region的memstores所占用内存达到Heap的35%时，不flush所有的memstore。它会找一个memstore内存占用最大的region，做个别flush，此时写更新还是会被block。lowerLimit算是一个在所有region强制flush导致性能降低前的补救措施。在日志中，表现为 “** Flushthread woke up with memory above low water.”。

调优：这是一个Heap内存保护参数，默认值已经能适用大多数场景。

（10）hfile.block.cache.size：默认20%

  这是涉及hbase读取文件的主要配置，BlockCache主要提供给读使用。读请求先到memstore中查数据，查不到就到blockcache中查，再查不到就会到磁盘上读，并把读的结果放入blockcache。由于blockcache是一个LRU,因此blockcache达到上限(heapsize * hfile.block.cache.size)后，会启动淘汰机制，淘汰掉最老的一批数据。对于注重读响应时间的系统，应该将blockcache设大些，比如设置blockcache=0.4，memstore=0.39，这会加大缓存命中率。

（11）hbase.regionserver.hlog.blocksize和hbase.regionserver.maxlogs

之所以把这两个值放在一起，是因为WAL的最大值由hbase.regionserver.maxlogs*hbase.regionserver.hlog.blocksize (2GB by default)决定。一旦达到这个值，Memstore flush就会被触发。所以，当你增加Memstore的大小以及调整其他的Memstore的设置项时，你也需要去调整HLog的配置项。否则，WAL的大小限制可能会首先被触发，因而，你将利用不到其他专门为Memstore而设计的优化。抛开这些不说，通过WAL限制来触发Memstore的flush并非最佳方式，这样做可能会会一次flush很多Region，尽管“写数据”是很好的分布于整个集群，进而很有可能会引发flush“大风暴”。

提示：最好将hbase.regionserver.hlog.blocksize* hbase.regionserver.maxlogs 设置为稍微大于hbase.regionserver.global.memstore.lowerLimit* HBASE_HEAPSIZE。

6.HDFS优化部分

HBase是基于hdfs文件系统的一个数据库，其数据最终是写到hdfs中的，因此涉及hdfs调优的部分也是必不可少的。

（1）dfs.replication.interval:默认3秒

可以调高，避免hdfs频繁备份，从而提高吞吐率。

（2）dfs.datanode.handler.count:默认为10

可以调高这个处理线程数，使得写数据更快

（3）dfs.namenode.handler.count：默认为8

（4）dfs.datanode.socket.write.timeout：默认480秒，并发写数据量大的时候可以调高一些，否则会出现我另外一篇博客介绍的的错误。

（5）dfs.socket.timeout:最好也要调高，默认的很小。

同上，可以调高，提高整体速度与性能。



1. zookeeper.session.timeout
regionserver在zookeeper的会话过期时间，默认是3分钟，如果regionserver 在zookeeper.session.timeout这个配置的时间没有去连zookeeper的话，zookeeper会将该regionserver在zookeeper摘除，该regionserver停止服务，很多情况下该值设置很大，原因是生产环境中regionserver的内存都配置很大，以扩大memstore和cache的大小，提高性能，但是内存配置大了，regionserver在jvm做一次内存大回收时，时间也会变长，很有可能这个时间超过zookeeper.session.timeout时间，导致regionserver在jvm回收内存的时候，zookeeper误以为regionserver挂掉而将regionserver摘除。该值还是不要配的过大，首先地java已支持cms方式回收内存，每次内存回收的时间不是太长，并且生产环境中，不允许过长时间的服务中断，配置大了，容易造成一个regionserver的服务真出现异常时，zookeeper不会切除该regionserver，使得很多请求失败

2. hbase.regionserver.handler.count
RegionServer端开启的RPC监听器实例个数，也即RegionServer能够处理的IO请求线程数。默认是10.
此参数与内存息息相关。该值设置的时候，以监控内存为主要参考。对于单次请求内存消耗较高的Big PUT场景（大容量单次PUT或设置了较大cache的scan，均属于Big PUT）或ReigonServer的内存比较紧张的场景，可以设置的相对较小。对于单次请求内存消耗低，TPS（TransactionPerSecond，每秒事务处理量）要求非常高的场景，可以设置的相对大些.通常都调到100~200之间，提高regionserver性能

3. hbase.regionserver.lease.period
regionserer租约时间，默认值是60s，如果生产环境中，在执行一些任务时，如mapred时出现lease超时的报错，那这个时候就需要去调大这个值了

4. file.block.cache.size
regionserver cache的大小，默认是0.2，是整个堆内存的多少比例作为regionserver的cache，调大该值会提升查询性能，当然也不能过大，如果你的hbase都大量的查询，写入不是很多的话，调到0.5也就够了，说到这个值，有一个地方需要说明一下，如果生产环境有mapred任务去scan hbase的时候，一些要在mapred scan类中加一个scan.setCacheBlocks(false)，避免由于mapred使用regionserver的cache都被替换，造成hbase的查询性能明显下降

5. hbase.hregion.max.filesize 默认值是256M
当HStoreFile的值超过这个值，就会引发HRegion的split。HBase中数据一开始会写入memstore，当memstore达到一定阈值后，会flush到disk上而成为storefile。当storefile数量超过3时，会启动compaction过程将它们合并为一个storefile。这个过程中会删除一些timestamp过期的数据，比如update的数据。而当合并后的storefile大小大于hfile默认最大值时，会触发split动作，将它切分成两个region。持续的insert压力测试得出：值越小，平均吞吐量越大，但吞吐量越不稳定；值越大，平均吞吐量越小，吞吐量不稳定的时间相对更小。
当hbase.hregion.max.filesize比较小时，触发split的机率更大，而split的时候会将region offline，因此在split结束的时间前，访问该region的请求将被block住，客户端自我block的时间默认为1s。当大量的region同时发生split时，系统的整体访问服务将大受影响。因此容易出现吞吐量及响应时间的不稳定现象
当hbase.hregion.max.filesize比较大时，单个region中触发split的机率较小，大量region同时触发split的机率也较小，因此吞吐量较之小hfile尺寸更加稳定些。但是由于长期得不到split，因此同一个region内发生多次compaction的机会增加了。compaction的原理是将原有数据读一遍并重写一遍到hdfs上，然后再删除原有数据。无疑这种行为会降低以io为瓶颈的系统的速度，因此平均吞吐量会受到一些影响而下降
综合以上两种情况，hbase.hregion.max.filesize不宜过大或过小，256MB或许是一个更理想的经验参数。对于离线型的应用，调整为128MB会更加合适。配置这个值，需要参考一下，平均每个regionserver管理的region数量，如果每台regionsever管理的region不多的话，可以适当的调大该值，如512M时再flush，而在线应用除非对split机制进行改造，否则不应该低于256MB

6. hbase.regionserver.global.memstore.upperLimit/hbase.regionserver.global.memstore.lowerLimit
配置一台regionserver所有memstore占整个堆的最大比例，默认是0.4/0.35，二个值的差异在于是做局部的flush，还是全部flush，如果你的regionserver日志中，频发出现因为超过hbase.regionserver.global.memstore.lowerLimit而做flush的信息，我觉得有必要调小hbase.hregion.memstore.flush.size，或者适当调大这二个值，当然hbase.regionserver.global.memstore.upperLimit和hfile.block.cache.size的和不能大于1，到0.8我觉得已经够大了。如果你的jvm内存回收是使用cms的话，有一个值CMSInitiatingOccupancyFraction（内存使用到时多少时，一始cms回收内存）的大小和觉得和这个有关系，略小于hbase.regionserver.global.memstore.upperLimit和hfile.block.cache.size的和是一个不错的选择。

7. hbase.hstore.compactionThreshold/hbase.hregion.majorcompaction
hbase.hstore.compactionThreshold执行compaction的store数量，默认值是3，如果需要提高查询性能，当然是storefile的数量越小，性能越好，但是执行compaction本身有性能资源的开消，如果regionserver频繁在compacion对性能影响也很大。hbase.hregion.majorcompaction表示majorcompaction的周期，默认是1天，majorcompaction与普通的compaction的区别是majorcompaction会清除过期的历史版本数据，同时合并storefile，而普通的compaction只做合并，通常都是majorcompaction，调为0，然后手工定期的去执行一下majorcompaction，适当调小点compacionThreshold。

8. hbase.regionserver.hlog.blocksize/hbase.regionserver.maxlogs
当数据被写入时会默认先写入Write-ahead Log(WAL)。WAL中包含了所有已经写入Memstore但还未Flush到HFile的更改(edits)。在Memstore中数据还没有持久化，当RegionSever宕掉的时候，可以使用WAL恢复数据。
当WAL(在HBase中成为HLog)变得很大的时候，在恢复的时候就需要很长的时间。因此，对WAL的大小也有一些限制，当达到这些限制的时候，就会触发Memstore的flush。Memstore flush会使WAL 减少，因为数据持久化之后(写入到HFile)，就没有必要在WAL中再保存这些修改。有两个属性可以配置：
你可能已经发现，WAL的最大值由hbase.regionserver.maxlogs * hbase.regionserver.hlog.blocksize (2GB by default)决定。一旦达到这个值，Memstore flush就会被触发。所以，当你增加Memstore的大小以及调整其他的Memstore的设置项时，你也需要去调整HLog的配置项。否则，WAL的大小限制可能会首先被触发，因而，你将利用不到其他专门为Memstore而设计的优化。抛开这些不说，通过WAL限制来触发Memstore的flush并非最佳方式，这样做可能会会一次flush很多Region，尽管“写数据”是很好的分布于整个集群，进而很有可能会引发flush“大风暴”。最好将hbase.regionserver.hlog.blocksize * hbase.regionserver.maxlogs 设置为稍微大于hbase.regionserver.global.memstore.lowerLimit * HBASE_HEAPSIZE.

对Memstore Flush来说，主要有两组配置项：决定Flush触发时机,决定Flush何时触发并且在Flush时候更新被阻断(block)
9. hbase.hregion.memstore.flush.size/hbase.regionserver.global.memstore.lowerLimit
关于触发“普通”flush，这类flush发生时，并不影响并行的写请求。需要注意的是第一个设置是每个Memstore的大小，当你设置该配置项时，你需要考虑一下每台RS承载的region总量。可能一开始你设置的该值比较小，后来随着region增多，那么就有可能因为第二个设置原因Memstore的flush触发会变早许多.

10.hbase.hregion.memstore.block.multiplier/hbase.regionserver.global.memstore.upperLimit
有时候集群的“写负载”非常高，写入量一直超过flush的量，这时，我们就希望memstore不要超过一定的安全设置。在这种情况下，写操作就要被阻止(blocked)一直到memstore恢复到一个“可管理”(manageable)的大小。某个节点“写阻塞”对该节点来说影响很大，但是对于整个集群的影响更大。HBase设计为：每个Region仅属于一个RS但是“写负载”是均匀分布于整个集群(所有Region上)。有一个如此“慢”的节点，将会使得整个集群都会变慢(最明显的是反映在速度上)。密切关注Memstore的大小和Memstore Flush Queue的大小。理想情况下，Memstore的大小不应该达到hbase.regionserver.global.memstore.upperLimit的设置，Memstore Flush Queue 的size不能持续增长


#Todo

    <property>

      <name>dfs.domain.socket.path</name>

      <value>/var/lib/hadoop-hdfs/dn_socket</value>

    </property>

   

#Todo

    <property>

      <name>hbase.bulkload.staging.dir</name>

      <value>/apps/hbase/staging</value>

    </property>

 

    #每条记录的最大大小为1MB

    <property>

      <name>hbase.client.keyvalue.maxsize</name>

      <value>1048576</value>

    </property>

 

#hbase client操作失败重新请求数为35   

    <property>

      <name>hbase.client.retries.number</name>

      <value>35</value>

    </property>

 

    #当一次scan操作不在本地内存时，需要从disk中获取时，缓存的条数，这里设置为100000条，该值不能大于下文中hbase.client.scanner.timeout.period配置项的值

    <property>

      <name>hbase.client.scanner.caching</name>

      <value>100000</value>

    </property>

  <property>

      <name>hbase.client.scanner.timeout.period</name>

      <value>120000</value>

    </property>

   

#hbase是否配置为分布式

    <property>

      <name>hbase.cluster.distributed</name>

      <value>true</value>

    </property>

   

#Todo

    <property>

      <name>hbase.coprocessor.master.classes</name>

      <value></value>

    </property>

   

#Todo

    <property>

      <name>hbase.coprocessor.region.classes</name>

      <value>org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint</value>

    </property>

   

#设置为ture，忽略对默认hbase版本的检查（设置为false的话在maven工程的编译过程中可能会遇到版本相关的问题）

    <property>

      <name>hbase.defaults.for.version.skip</name>

      <value>true</value>

    </property>

   

#设置系统进行1次majorcompaction的启动周期，如果设置为0，则系统不会主动出发MC过程，默认为7天

    <property>

      <name>hbase.hregion.majorcompaction</name>

      <value>604800000</value>

    </property>

   

#用来作为计算MC时间周期，与hbase.hregion.majorcompaction相结合，计算出一个浮动的MC时间。默认是0.50，简单来说如果当前store中hfile的最早更新时间早于某个MCTime，就会触发major compaction，hbase通过这种机制定期删除过期数据。MCTime是一个浮动值，浮动区间为[ hbase.hregion.majorcompaction - hbase.hregion.majorcompaction * hbase.hregion.majorcompaction.jitter , hbase.hregion.majorcompaction + hbase.hregion.majorcompaction * hbase.hregion.majorcompaction.jitter ]

    <property>

      <name>hbase.hregion.majorcompaction.jitter</name>

      <value>0.50</value>

    </property>

 

    #单个region的大小为10G，当region大于这个值的时候，一个region就会split为两个，适当的增加这个值的大小可以在写操作时减少split操作的发生，从而减少系统性能消耗而增加写操作的性能，默认是10G，官方建议10G~30G

    <property>

      <name>hbase.hregion.max.filesize</name>

      <value>10737418240</value>

    </property>

 

    #当一个region的memstore总量达到hbase.hregion.memstore.block.multiplier * hbase.hregion.memstore.flush.size (默认2*128M)时，会阻塞这个region的写操作，并强制刷写到HFile，触发这个刷新操作只会在Memstore即将写满hbase.hregion.memstore.flush.size时put了一个巨大的记录的情况，这时候会阻塞写操作，强制刷新成功才能继续写入

    <property>

      <name>hbase.hregion.memstore.block.multiplier</name>

      <value>8</value>

    </property>

   

#每个单独的memstore的大小（默认128M），这里调成了256M，每个列族columnfamily在每个region中都分配有它单独的memstore，当memstore超过该值时，就会发生flush操作，将memstore中的内容刷成一个hfile，每一次memstore的flush操作，都会为每一次columnfamily创建一个新的hfile；调高该值可以减少flush的操作次数，减少每一个region中的hfile的个数，这样就会减少minor compaction的次数和split的次数，从而降低了系统性能损耗，提升了写性能，也提升了读性能（因为读操作的时候，首先要去memstore中查数据，查找不到的话再去hfile，hflie存储在hdfs中，这就涉及到了对性能要求较高的io操作了）。当然这个值变大了之后，每次flush操作带来的性能消耗也就更大。

    <property>

      <name>hbase.hregion.memstore.flush.size</name>

      <value>268435456</value>

    </property>

   

#mslab特性是在分析了HBase产生内存碎片后的根因后给出了解决方案，这个方案虽然不能够完全解决Full GC带来的问题，但在一定程度上延缓了Full GC的产生间隔，总之减少了内存碎片导致的full gc，提高整体性能。

    <property>

      <name>hbase.hregion.memstore.mslab.enabled</name>

      <value>true</value>

    </property>

   

#当任意一个store中有超过hbase.hstore.blockingStoreFiles个数的storefiles时，这个store所在region的update操作将会被阻塞，除非这个region的compaction操作完成或者hbase.hstore.blockingWaitTime超时。

Block操作会严重影响当前regionserver的响应时间，但过多的storefiles会影响读性能，站在实际使用的角度，为了获取较为平滑的响应时间，可以将该值设得很大，甚至无限大。默认值为7，这里暂时调大到100。

    <property>

      <name>hbase.hstore.blockingStoreFiles</name>

      <value>100</value>

    </property>

   

#一次minor compaction的最大file数

    <property>

      <name>hbase.hstore.compaction.max</name>

      <value>10</value>

    </property>

   

#一次minor compaction的最小file数

    <property>

      <name>hbase.hstore.compactionThreshold</name>

      <value>4</value>

    </property>

 

#本地文件目录用来作为hbase在本地的存储   

    <property>

      <name>hbase.local.dir</name>

      <value>${hbase.tmp.dir}/local</value>

    </property>

 

#todo

#与前文配置项图中第二红线标注的配置项重复

    <property>

      <name>hbase.master.distributed.log.splitting</name>

      <value>ture</value>

    </property>

   

#hbase master web界面绑定的IP地址（任何网卡的ip都可以访问）

    <property>

      <name>hbase.master.info.bindAddress</name>

      <value>0.0.0.0</value>

    </property>

   

#hbase master web界面绑定端口

    <property>

      <name>hbase.master.info.port</name>

      <value>16010</value>

    </property>

   

#todo

    <property>

      <name>hbase.master.port</name>

      <value>16000</value>

    </property>

   

#分配1%的regionserver的内存给写操作当作缓存，这个参数和下面的hfile.block.cache.size（读缓存）息息相关，二者之和不能超过总内存的80%，读操作时，该值最好为0，但是这里有个bug，取不到0，所以取值1%即0.01，系统尽可能的把内存给读操作用作缓存

    <property>

      <name>hbase.regionserver.global.memstore.size</name>

      <value>0.01</value>

    </property>

   

#regionserver处理IO请求的线程数，默认是30这里调高到240

    <property>

      <name>hbase.regionserver.handler.count</name>

      <value>240</value>

    </property>

 

#regionserver 信息 web界面接口

    <property>

      <name>hbase.regionserver.info.port</name>

      <value>16030</value>

    </property>

 

#regionserver服务端口

    <property>

      <name>hbase.regionserver.port</name>

      <value>16020</value>

    </property>

   

#todo

    <property>

      <name>hbase.regionserver.wal.codec</name>

      <value>org.apache.hadoop.hbase.regionserver.wal.WALCellCodec</value>

    </property>

   

#hbase所有表的文件存放在hdfs中的路径，用户可以在hdfs的web页面和后台命令行中查看，若要彻底删除表，现在hbase中删除，然后在hdfs中删除源文件即可，drop命令运行过后hdfs上内容没有删除情况下。

    <property>

      <name>hbase.rootdir</name>

      <value>hdfs://node1.dcom:8020/apps/hbase/data</value>

    </property>

 

#todo   

    <property>

      <name>hbase.rpc.protection</name>

      <value>authentication</value>

    </property>

   

#hbase rpc操作超时时间

    <property>

      <name>hbase.rpc.timeout</name>

      <value>90000</value>

    </property>

 

#todo

    <property>

      <name>hbase.security.authentication</name>

      <value>simple</value>

    </property>

   

       #todo

    <property>

      <name>hbase.security.authorization</name>

      <value>false</value>

    </property>

   

#todo

    <property>

      <name>hbase.superuser</name>

      <value>hbase</value>

    </property>

   

#本地文件系统上的临时目录，最好不要使用/tmp下的目录，以免重启后丢失文件

    <property>

      <name>hbase.tmp.dir</name>

      <value>/tmp/hbase-${user.name}</value>

    </property>

   

#zookeeper配置文件zoo.cfg中定义的内容，zookeeper 客户端通过该port连接上zookeeper服务

    <property>

      <name>hbase.zookeeper.property.clientPort</name>

      <value>2181</value>

    </property>

   

#zookeeper服务的节点数目和各节点名称

    <property>

      <name>hbase.zookeeper.quorum</name>

      <value>node1.dcom,node2.dcom,node3.dcom</value>

    </property>

   

#zookeeper支持多重update

    <property>

      <name>hbase.zookeeper.useMulti</name>

      <value>true</value>

    </property>

   

    #将regionserver的内存的79%分配作为读缓存，默认是40%，这里因为是单独的读操作性能调优所以调到了79%，上文中提到了一个bug，不能调为最高的80%。该配置项与上文中的hbase.regionserver.global.memstore.size关系密切，二者的总和不能大于regionserver内存的80%，读操作为主时就将该值调高，写操作为主时就将hbase.regionserver.global.memstore.size调高

    <property>

      <name>hfile.block.cache.size</name>

      <value>0.79</value>

    </property>

 

#todo

    <property>

      <name>phoenix.query.timeoutMs</name>

      <value>60000</value>

    </property>

   

#zookeeper session会话超时时间

    <property>

      <name>zookeeper.session.timeout</name>

      <value>90000</value>

    </property>

   

#znode 存放root region的地址

#todo

    <property>

      <name>zookeeper.znode.parent</name>

      <value>/hbase-unsecure</value>

    </property>

   

  </configuration>

 

# RegionServers maximum value for –Xmn 新生代jvm内存大小，默认是1024，这里调到了4096，这个参数影响到regionserver 的jvm的CMS  GC，64G内存的话建议1~3G，最大为4G，regionserver –Xmn in –Xmx ratio配置项也密切相关，该比例设置的太大或者太小都不好，这方面涉及到的内容太多，后续再详细介绍。

# Number of Fetched Rows when Scanning from Disk这个就是上文中提到的hbase.client.scanner.caching

# Maximum Store Files before Minor Compaction 在执行Minor Compaction合并操作前Store Files的最大数目，默认是3，这里调到了4


<!-- 分配的内存大小尽可能的多些，前提是不能超过 (机器实际物理内存-JVM内存) -->
<property>  
   <name>hbase.bucketcache.size</name>  
   <value>16384</value> 
</property>
<property>
 <name>hbase.bucketcache.ioengine</name>
 <value>offheap</value> 
</property>


//快照 这样试试，先建立个表自己测试下，可以的话在执行。 需要开启快照功能，在hbase-site.xml文件中添加如下配置项：
 <property>
 <name>hbase.snapshot.enabled</name>
 <value>true</value>
 </property>
//命令
 hbase shell> disable 'tableName' 
 hbase shell> snapshot 'tableName', 'tableSnapshot' 
 hbase shell> clone_snapshot 'tableSnapshot', 'newTableName' 
 hbase shell> delete_snapshot 'tableSnapshot' 
 hbase shell> drop 'tableName'


